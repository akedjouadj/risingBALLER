{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### Notebook for straightforward masked players prediction\n","\n","I highly recommend to run this on google colab"]},{"cell_type":"markdown","metadata":{},"source":["1. First connect the notebooks to your google drive to load data and save outputs directly there."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24238,"status":"ok","timestamp":1728379908722,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"TrypPmtahrkX","outputId":"75891117-cd07-432d-f047-2888d901951c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":46356,"status":"ok","timestamp":1728379955074,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"ve2fW7tPhQW7"},"outputs":[],"source":["import json\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import Trainer, TrainingArguments\n","from transformers.modeling_outputs import MaskedLMOutput\n","\n","from safetensors.torch import load_file\n","\n","from prettytable import PrettyTable\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"VYy_f-VbhQW8"},"source":["2. Prepare the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"executionInfo":{"elapsed":1058,"status":"ok","timestamp":1728379956129,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"eE0jD_4dyH4D","outputId":"393beafa-44ba-4023-aaaa-77a7c110a758"},"outputs":[],"source":["import pandas as pd\n","\n","df_input = pd.read_csv('../dataset/statsbomb/df_raw_counts_players_matches.csv') # consider replacing by the right path in your folder\n","df_input"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1728379957487,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"P-m3-3QW26SX","outputId":"fb9d5f8d-d70a-439e-862e-a5a8c3fe06f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["5106\n","141\n"]}],"source":["LABEL2PLAYER_NAME = dict(enumerate(df_input['player_name'].astype('category').cat.categories))\n","PLAYER_NAME2LABEL = {v: k for k, v in LABEL2PLAYER_NAME.items()}\n","\n","LABEL2TEAM_NAME = dict(enumerate(df_input['team_name'].astype('category').cat.categories))\n","TEAM_NAME2LABEL = {v: k for k, v in LABEL2TEAM_NAME.items()}\n","\n","print(len(PLAYER_NAME2LABEL))\n","print(len(TEAM_NAME2LABEL))"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1728379957488,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"uC92jhc0hQW_"},"outputs":[],"source":["PLAYER_NAME2LABEL['MASK'] = 5106\n","PLAYER_NAME2LABEL['PAD'] = 5107\n","LABEL2PLAYER_NAME[5106] = 'MASK'\n","LABEL2PLAYER_NAME[5107] = 'PAD'\n","\n","TEAM_NAME2LABEL['PAD'] = 141\n","LABEL2TEAM_NAME[141] = 'PAD'"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728379957488,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"K5xEB7q_Mf5f"},"outputs":[],"source":["TEAM_MAX_LENGTH = 18\n","ID_COLUMNS = ['player_name', 'team_name', 'competition_name', 'season_name', 'match_id', 'position_id', 'position_name']\n","FORM_STATS_SIZE = df_input.shape[1]-len(ID_COLUMNS)-1 # -1 remove is_aligned\n","\n","MAX_PLAYER_IDX = df_input.player_name.nunique()-1\n","MAX_TEAM_IDX = df_input.team_name.nunique()-1\n","MAX_POSITION_IDX = 24\n","\n","PLAYER_PAD_TOKEN_ID=MAX_PLAYER_IDX+1\n","PLAYER_MASK_TOKEN_ID=MAX_PLAYER_IDX+2\n","TEAM_PAD_TOKEN_ID=MAX_TEAM_IDX+1\n","POSITION_PAD_TOKEN_ID = MAX_POSITION_IDX+1\n","\n","PLAYERS_BANK_SIZE = MAX_PLAYER_IDX+1+1\n","TEAMS_BANK_SIZE = MAX_TEAM_IDX+1\n","POSITION_BANK_SIZE = MAX_POSITION_IDX+1"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1728379957759,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"ohXqLmJ_hQW_"},"outputs":[],"source":["class DataCollatorMaskedPM(Dataset):\n","    def __init__(self,\n","                 df_input,\n","                 player_pad_token_id=PLAYER_PAD_TOKEN_ID,\n","                 player_mask_token_id=PLAYER_MASK_TOKEN_ID,\n","                 team_pad_token_id=TEAM_PAD_TOKEN_ID,\n","                 position_pad_toekn_id = POSITION_PAD_TOKEN_ID,\n","                 player_name2label=PLAYER_NAME2LABEL,\n","                 team_name2label=TEAM_NAME2LABEL,\n","                 mask_percentage = 0.25):\n","\n","        self.df_input = df_input\n","        self.player_pad_token_id = player_pad_token_id\n","        self.player_mask_token_id = player_mask_token_id\n","        self.team_pad_token_id = team_pad_token_id\n","        self.position_pad_token_id = position_pad_toekn_id\n","        self.player_name2label = player_name2label\n","        self.team_name2label = team_name2label\n","        self.max_length = 2*TEAM_MAX_LENGTH\n","        self.mask_percentage = mask_percentage\n","\n","    def __len__(self):\n","        return self.df_input.match_id.nunique()\n","\n","    def mask_players(self, match_input_player_id, match_output_player_id, match_input_form_stats, match_attention_mask, player_mask_token_id, mask_percentage):\n","\n","        # mask between only the players tokens, not the padding tokens\n","        maskable_idx = np.where(match_attention_mask == 1)[0]\n","\n","        number_masked_players = int(len(maskable_idx)*mask_percentage)\n","\n","        masked_idx = np.random.choice(maskable_idx, number_masked_players, replace=False)\n","        non_masked_idx = [idx for idx in range(len(match_input_player_id)) if idx not in masked_idx]\n","\n","        match_input_player_id[masked_idx] = player_mask_token_id\n","        match_input_form_stats[masked_idx] = 0\n","        match_output_player_id[non_masked_idx] = -100\n","\n","        return match_input_player_id, match_output_player_id, match_input_form_stats\n","\n","    def __getitem__(self, idx):\n","\n","        \"\"\"\n","        idx is the idx of an element in the dataset, a number between 0 and len(dataset)\n","        \"\"\"\n","        #print(f\"idx: {idx}\")\n","        match_id = self.df_input.match_id.unique()[idx]\n","        match_input = self.df_input[self.df_input.match_id == match_id]\n","\n","        match_teams = match_input.team_name.unique()\n","        match_input = pd.concat([match_input[match_input.team_name == match_teams[i]] for i in range(2)], ignore_index=True) # ensure that the players name in the same order as the input\n","\n","        if len(match_teams) != 2:\n","            print (f\"Error: match {match_id} contains {len(match_teams)} teams !\")\n","            return None\n","\n","        # encode the player_name to player_id\n","        match_input_player_name = match_input.player_name\n","        match_input_player_id = np.array([self.player_name2label[player_name] for player_name in match_input_player_name])\n","        match_input_player_id = np.pad(match_input_player_id, (0, self.max_length-len(match_input_player_id)), mode='constant', constant_values=self.player_pad_token_id)\n","\n","        match_output_player_name = match_input.player_name\n","        match_output_player_id = np.array([self.player_name2label[player_name] for player_name in match_output_player_name])\n","        match_output_player_id = np.pad(match_output_player_id, (0, self.max_length-len(match_output_player_id)), mode='constant', constant_values=self.player_pad_token_id)\n","\n","        # encode the team_name to team_id\n","        match_input_team_name = match_input.team_name\n","        match_input_team_id = [self.team_name2label[team_name] for team_name in match_input_team_name]\n","        match_input_team_id = np.pad(match_input_team_id, (0, self.max_length-len(match_input_team_id)), mode='constant', constant_values=self.team_pad_token_id)\n","\n","        # spatial position id\n","        match_input_position_id = np.array(match_input.position_id)\n","        match_input_position_id = np.pad(match_input_position_id, (0, self.max_length-len(match_input_position_id)), mode='constant', constant_values=self.position_pad_token_id)\n","\n","        # remove the id columns\n","        match_input = match_input.drop(columns=ID_COLUMNS, axis=1)\n","\n","        # add the attention mask depending on if the player is playing or not\n","        match_attention_mask = np.array(match_input.is_aligned)\n","        match_attention_mask = np.pad(match_attention_mask, (0, self.max_length-len(match_attention_mask)), mode='constant', constant_values=0)\n","\n","        match_input = match_input.drop(columns=['is_aligned'], axis=1)\n","\n","        # prepare the players form stats (TPE) for each player\n","        match_input_form_stats = np.array(match_input)\n","        match_input_form_stats = np.pad(match_input_form_stats, ((0, self.max_length-match_input_form_stats.shape[0]), (0, 0)), mode='constant', constant_values=0)\n","\n","        # masking strategy, 25% of the players that are playing are masked, means 7/28 players per match\n","        match_input_player_id, match_output_player_id, match_input_form_stats = self.mask_players(match_input_player_id, match_output_player_id, match_input_form_stats, match_attention_mask,\n","                                                                                                  self.player_mask_token_id, self.mask_percentage)\n","        # return the dict of input and output data\n","        sample = {\n","                  'input_ids': torch.tensor(match_input_player_id, dtype=torch.long),\n","                  'labels': torch.tensor(match_output_player_id, dtype=torch.long),\n","                  'position_id': torch.tensor(match_input_position_id, dtype=torch.long),\n","                  'team_id': torch.tensor(match_input_team_id, dtype=torch.long),\n","                  'form_stats': torch.tensor(match_input_form_stats).float(),\n","                  'attention_mask': torch.tensor(match_attention_mask, dtype=torch.long),\n","                  }\n","\n","        return sample\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1728379957759,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"5ILBXR0ChQXB","outputId":"1b24439d-a979-4168-d323-b007847e5546"},"outputs":[{"name":"stdout","output_type":"stream","text":["38\n"]}],"source":["def custom_collate_fn(batch):\n","    # Filtrer les éléments None\n","    batch = [item for item in batch if item is not None]\n","    return torch.utils.data.dataloader.default_collate(batch)\n","\n","batch_size = 64\n","\n","my_dataset = DataCollatorMaskedPM(df_input)\n","my_dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last = True)\n","\n","print(len(my_dataloader))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"collapsed":true,"executionInfo":{"elapsed":12290,"status":"error","timestamp":1728379970044,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"DuS2XTlSQKrq","outputId":"09e83c41-e191-41ee-eb9c-0a4f0cd12d1b"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:37<00:00, 18.55s/it]"]},{"name":"stdout","output_type":"stream","text":["\n"," tensor([5107, 1118, 4654, 3874, 1690, 2222, 5107, 5107, 1629, 4056, 3860, 1258,\n","        3742, 2827, 3902, 2100,  778, 2753, 4239, 5107, 3043, 1512, 5107, 1064,\n","        5107,  662, 1015, 5107, 5106, 5106, 5106, 5106, 5106, 5106, 5106, 5106])\n","tensor([4145, -100, -100, -100, -100, -100, 4600, 2786, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, 3448, -100, -100,  993, -100,\n","         456, -100, -100, 3642, -100, -100, -100, -100, -100, -100, -100, -100])\n","76\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["repeat = 2\n","all_batches = []\n","for count in tqdm(range(repeat)):\n","  # there will be a shuffling at each repetition and a different masking for each batch in the dataloader\n","  for batch in my_dataloader:\n","      all_batches.append(batch)\n","\n","print(\"\\n\", batch['input_ids'][10])\n","print(batch['labels'][10])\n","print(len(all_batches))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1728379976037,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"wplbIQDl7pH3"},"outputs":[],"source":["class PreprocessedDataCollatorMaskedPM(Dataset):\n","\n","    def __init__(self, all_batches):\n","      self.data = all_batches\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","\n","        \"\"\"\n","        idx an already preprocessed batch\n","        \"\"\"\n","\n","        return self.data[idx]"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"collapsed":true,"executionInfo":{"elapsed":538,"status":"error","timestamp":1727617856447,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"NRU0MBj4RDew","outputId":"0e501526-2449-40bc-9b36-ca93e07abf24"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 4 35 10]\n","73\n","3\n"]}],"source":["dev_size = int(0.05*len(all_batches))\n","\n","np.random.seed(42)\n","dev_batches_idx = np.random.choice(range(len(all_batches)), dev_size, replace=False)\n","print(dev_batches_idx)\n","train_batches_idx = [idx for idx in range(len(all_batches)) if idx not in dev_batches_idx]\n","\n","dev_batches = [all_batches[idx] for idx in dev_batches_idx]\n","train_batches = [all_batches[idx] for idx in train_batches_idx]\n","\n","batch_size = 1\n","\n","dataset_train = PreprocessedDataCollatorMaskedPM(train_batches)\n","dataset_val = PreprocessedDataCollatorMaskedPM(dev_batches)\n","\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n","dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n","\n","print(len(dataloader_train))\n","print(len(dataloader_val))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"collapsed":true,"executionInfo":{"elapsed":245,"status":"error","timestamp":1727617882011,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"rFpnopMXSpHx","outputId":"4d5f5124-8f29-49bf-b4c3-41e0e419081b"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/73 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["input_ids torch.Size([1, 64, 36])\n","labels torch.Size([1, 64, 36])\n","position_id torch.Size([1, 64, 36])\n","team_id torch.Size([1, 64, 36])\n","form_stats torch.Size([1, 64, 36, 39])\n","attention_mask torch.Size([1, 64, 36])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# observed the inputs shape\n","for batch in tqdm(dataloader_train):\n","    for key in batch:\n","        print(key, batch[key].shape)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"vvGUwBBshQXB"},"source":["##### model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":278,"status":"ok","timestamp":1728379982517,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"3NE8hy6FhQXC"},"outputs":[],"source":["class PlayerSelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(PlayerSelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size//heads\n","\n","        assert(self.head_dim*heads == embed_size), \"Embed size needs to be divisible by heads\"\n","\n","        # compute the values, keys and queries for all heads\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, queries, mask=None):\n","        N = queries.shape[0]\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n","\n","        # split embedding into self.heads pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        values = self.values(values)\n","        keys = self.keys(keys)\n","        queries = self.queries(queries)\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","\n","        if mask is not None:\n","            mask = mask.unsqueeze(1).unsqueeze(2).expand(N, 1, query_len, key_len)\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        attention = torch.softmax(energy/ (self.head_dim ** 0.5), dim = 3) # normalize accross the key_len\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim)\n","\n","        out = self.fc_out(out)\n","\n","        return out, attention\n","\n","class PlayerTransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion) :\n","        super(PlayerTransformerBlock, self).__init__()\n","        self.attention = PlayerSelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, embed_size*forward_expansion),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion*embed_size, embed_size)\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask=None):\n","        attention, attention_matrix = self.attention(value, key, query, mask)\n","        x = self.dropout(self.norm1(attention + query))\n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm2(forward + x))\n","        return out, attention_matrix\n","\n","\n","class PlayerEncoder(nn.Module):\n","    def __init__(self, embed_size, num_layers, heads, forward_expansion, dropout, form_stats_size,\n","                  players_bank_size, teams_bank_size, n_positions, use_teams_embeddings = False):\n","\n","        super(PlayerEncoder, self).__init__()\n","\n","        self.embed_size = embed_size\n","        self.use_teams_embeddings = use_teams_embeddings\n","\n","        self.form_embeddings = nn.Linear(form_stats_size, embed_size)\n","        self.players_embeddings = nn.Embedding(players_bank_size+1, embed_size, padding_idx = players_bank_size)\n","        if self.use_teams_embeddings:\n","            self.teams_embeddings = nn.Embedding(teams_bank_size+1, embed_size, padding_idx=teams_bank_size)\n","        self.positions_embeddings = nn.Embedding(n_positions+1, embed_size, padding_idx = n_positions)\n","\n","\n","        self.layers = nn.ModuleList([PlayerTransformerBlock(embed_size, heads, dropout, forward_expansion)\n","                                     for _ in range(num_layers)])\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, player_id, position_id, team_id, form_stats, attention_mask):\n","\n","        if self.use_teams_embeddings:\n","            out = self.dropout(self.relu(self.players_embeddings(player_id))+\\\n","                            self.form_embeddings(form_stats)+\\\n","                            self.teams_embeddings(team_id)+\\\n","                            self.positions_embeddings(position_id))\n","        \n","        else:\n","            out = self.dropout(self.relu(self.players_embeddings(player_id))+\\\n","                            self.form_embeddings(form_stats)+\\\n","                            self.positions_embeddings(position_id))\n","\n","        attention_matrices = []\n","        for layer in self.layers:\n","            out, attention_matrix = layer(out, out, out, attention_mask)\n","            attention_matrices.append(attention_matrix)\n","\n","        return out, attention_matrices\n","\n","class TransformerForMaskedPM(nn.Module):\n","    def __init__(self, embed_size, num_layers, heads, forward_expansion, dropout, form_stats_size= FORM_STATS_SIZE,\n","                  players_bank_size = PLAYERS_BANK_SIZE, teams_bank_size = TEAMS_BANK_SIZE,\n","                  n_positions = POSITION_BANK_SIZE):\n","        super(TransformerForMaskedPM, self).__init__()\n","\n","        self.players_bank_size = players_bank_size\n","\n","        self.player_encoder = PlayerEncoder(embed_size, num_layers, heads, forward_expansion, dropout, form_stats_size,\n","                                            players_bank_size, teams_bank_size, n_positions)\n","\n","        self.decoder = nn.Linear(embed_size, players_bank_size)\n","\n","        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n","\n","    def forward(self, input_ids, labels, position_id, team_id, form_stats, attention_mask):\n","\n","        input_ids = input_ids.squeeze(0)\n","        labels = labels.squeeze(0)\n","        position_id = position_id.squeeze(0)\n","        team_id = team_id.squeeze(0)\n","        form_stats = form_stats.squeeze(0)\n","        attention_mask = attention_mask.squeeze(0)\n","\n","        players_embeddings, attention_matrices = self.player_encoder(input_ids, position_id, team_id, form_stats, attention_mask)\n","\n","        output = self.decoder(players_embeddings)\n","        \n","        loss = self.criterion(output.view(-1, self.players_bank_size), labels.view(-1))\n","\n","\n","        return MaskedLMOutput(loss = loss,\n","                              logits = output,\n","                              hidden_states = players_embeddings,\n","                              attentions=attention_matrices)\n","\n","\n","def count_parameters(model, print_table = False):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad:\n","            continue\n","        params = parameter.numel()\n","        table.add_row([name, params])\n","        total_params += params\n","    if print_table:\n","        print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1728379987087,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"YNss9SBThQXC"},"outputs":[],"source":["model = TransformerForMaskedPM(embed_size=128,\n","                                num_layers=1,\n","                                heads=2,\n","                                forward_expansion=4,\n","                                dropout=0.05)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1728379988460,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"P0pkUkyPhQXD","outputId":"5cfebddf-0326-4725-971d-31a2e98aa554"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------+------------+\n","|                     Modules                      | Parameters |\n","+--------------------------------------------------+------------+\n","|      player_encoder.form_embeddings.weight       |    4992    |\n","|       player_encoder.form_embeddings.bias        |    128     |\n","|     player_encoder.players_embeddings.weight     |   653824   |\n","|    player_encoder.positions_embeddings.weight    |    3328    |\n","| player_encoder.layers.0.attention.values.weight  |    4096    |\n","|  player_encoder.layers.0.attention.keys.weight   |    4096    |\n","| player_encoder.layers.0.attention.queries.weight |    4096    |\n","| player_encoder.layers.0.attention.fc_out.weight  |   16384    |\n","|  player_encoder.layers.0.attention.fc_out.bias   |    128     |\n","|       player_encoder.layers.0.norm1.weight       |    128     |\n","|        player_encoder.layers.0.norm1.bias        |    128     |\n","|       player_encoder.layers.0.norm2.weight       |    128     |\n","|        player_encoder.layers.0.norm2.bias        |    128     |\n","|  player_encoder.layers.0.feed_forward.0.weight   |   65536    |\n","|   player_encoder.layers.0.feed_forward.0.bias    |    512     |\n","|  player_encoder.layers.0.feed_forward.2.weight   |   65536    |\n","|   player_encoder.layers.0.feed_forward.2.bias    |    128     |\n","|                  decoder.weight                  |   653696   |\n","|                   decoder.bias                   |    5107    |\n","+--------------------------------------------------+------------+\n","Total Trainable Params: 1482099\n"]},{"data":{"text/plain":["1482099"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(model, print_table=True)"]},{"cell_type":"markdown","metadata":{"id":"4Ez5okCUWdEH"},"source":["### Training"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"zW6ITZssACH2"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","\n","    model_output, labels = eval_pred # labels: [len(dev_batches) 14, sample_bs 256, n_players 80]\n","    pred, players_embeddings, attention_matrices = model_output # pred: [14*256, n_players, players_bank_size]\n","\n","    #print(labels.shape, pred.shape)\n","\n","    len_dev_batches, bs_sample, n_players = labels.shape\n","\n","    labels = labels.reshape(len_dev_batches*bs_sample, n_players)\n","    labels = labels.reshape(len_dev_batches*bs_sample*n_players,)\n","\n","    pred = pred.reshape(pred.shape[0]*n_players, -1)\n","\n","    # remove the padding tokens\n","    mask_non_pad_players = labels != -100\n","    labels = labels[mask_non_pad_players]\n","    pred = pred[mask_non_pad_players]\n","\n","    #print(labels.shape)\n","    #print(pred.shape)\n","\n","    # find the most likely predicted player\n","    pred_top1_idx = np.argmax(pred, axis=1)\n","\n","    # find the top 3 most likely predicted players\n","    pred_top3_idx = np.argsort(pred, axis=1)[:, -3:]\n","\n","    # compute the model top 1 accuracy\n","    accuracy_top1 = (labels==pred_top1_idx).mean()\n","\n","    # compute the model top3 accuracy\n","    accuracy_top3 = 0\n","    for label, pred_top3 in zip(labels, pred_top3_idx):\n","        if label in pred_top3:\n","            accuracy_top3 += 1\n","    accuracy_top3 /= len(labels)\n","\n","    outputs = {'accuracy_top1': accuracy_top1,\n","               'accuracy_top3': accuracy_top3}\n","\n","\n","    return outputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1727606723386,"user":{"displayName":"annotateur lambda1","userId":"15649786041223544994"},"user_tz":-120},"id":"ivBFWp4khQXD","outputId":"277475f0-ee91-44d3-de6b-ef3794ec30bc"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='outputs/masked_players_prediction',\n","    num_train_epochs=2000, # 342K steps\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    report_to=\"tensorboard\",\n","    learning_rate=1e-4,\n","    warmup_ratio=0,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=100, # every 50 epochs\n","    logging_strategy=\"steps\",\n","    logging_steps=100, # every 50 epochs\n","    save_strategy = \"steps\",\n","    save_steps = 42750, # every 250 epochs\n",")\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset_train,\n","    eval_dataset=dataset_val,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"u9Qq22iWb8X0","outputId":"e51c4fb9-e12a-4a20-9163-f456004bd12b"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["#### Save embeddings"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"load my pretrained checkpoint (the one behing RisB-Profiler), trained for 2000 epochs, \n","342K steps on the statsbomb dataset without the teams embeddings \n","\"\"\"\n","state_dict = load_file('pretrained_ckpt/model.safetensors')\n","\n","model.load_state_dict(state_dict)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"EAbUTtcMRKdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["(25, 128) (5106, 128)\n"]}],"source":["positions_embeddings = model.state_dict()['player_encoder.positions_embeddings.weight'].numpy()[:-1] # remove the padding token\n","players_embeddings = model.state_dict()['decoder.weight'].numpy()[:-1] # remove the padding token\n","\n","print(positions_embeddings.shape, players_embeddings.shape)\n","\n","np.save('pretrained_ckpt/players_embeddings_1l128d2h_wo_teams_emb_statsbomb_2454games.npy', players_embeddings)\n","np.save('pretrained_ckpt/positions_embeddings_1l128d2h_wo_teams_emb_statsbomb_2454games.npy', positions_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
